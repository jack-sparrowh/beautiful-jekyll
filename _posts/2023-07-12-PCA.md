---
layout: post
title: Solving PCA without matrix calculus and Lagrange multipliers
subtitle: We show that you don't need calculus
tags: [test, PCA, linear algebra]
---

### 1. Introduction
Do you know what PCA is and does but you are unable to derive the results by yourself? Maybe you are stuck because of matrix calculus or maybe you don't know how to use Lagranglee multipliers, but you are just like me and the best way of understanding things  are by deriving them yourself. If you understand linear algebra the PCA can be solved without calculus. I will be using different theorems, that will be proven at the end of this blog, my knowledge on how to solve this problem comes from the great "Linear Algebra and its applications" by David C. Lay[1], which i highly recommend. I was trying to find similar articles or procedures, but with no success. If you know some that has taken the same route as in this blog, please link it in the comments and i will put it in the references.
{: style="text-align: justify"}

#### 1.1. Notation
Vectors are denoted by bfface lowercase letters like $$\bf{x}$$, where we're using the convention that vectors are column vectors. We assume that any arbitrary vector comes from $$\mathbb{R}^{n}$$. Matrices are denoted by bfface uppercase letters like $$\bf{X}$$. Scalar values are denoted as usual. Random variables are denoted by uppercase letters like $$X$$ and random vectors are denoted as $$\mathbb{X}$$. The expected value w.r.t. random variable $$X$$ is denoted as $$\mathbb{E}_{X}[ \cdot ]$$. The $$\mathbb{l}2$$-norm is denoted as $$\Vert \bf{x} \Vert_{2}$$ and is defined as an inner product $$\langle\bf{x}, \bf{x}\rangle = \bf{x}^{T}\bf{x}$$.
{: style="text-align: justify"}

### 2. Outline of the objective problem
The basic premise behind PCA is to project some high-dimensional data onto a subspace generated by a set of vectors. We can define the objective function of PCA in two ways, where the second (*maximization*) comes naturally from the first (*minimization*), and so we will start by outlining the most basic definition. At the beginning we only consider one data point (a single vector) $$\bf{x} \in \mathbb{R}^{n}$$ to build some intuition and to simplify the calculations. Then, we can take any arbitrarily big dataset $$\bf{X} \in \mathbb{R}^{m \times n}$$ and proceed in the similar manner. We will also only take a look at projecting our datapoint onto a 1-dimentional subspace. Projecting onto a higher dimensional subspaces will be described later.
{: style="text-align: justify"}

The PCA objective function tries to minimize the distance between our data vector $$\bf{x}$$ and the subspace spanned by some vector $$\bf{v} \in \mathbb{R}^{n}$$, which we denote as $$Span\{\bf{v}\}$$. We put a constraint, that $$\bf{v}$$ must be a unit vector, as this will only assure the unique solution. Since, minimizing the squared distance will give us the same result, we will be using it to write our objective, which turns out to be:
{: style="text-align: justify"}

$$
\begin{aligned}
 (\hat{\bf{v}}, \hat{\alpha}):= 
 \underset{
	\bf{v}, \ \alpha, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmin}}
	\Vert\bf{x}-\alpha \bf{v}\Vert_{2}^2 \\
 \end{aligned}
 $$ 

### 3. Solving 
#### 3.1. Solving for $$\alpha$$ 
Normally we would start with solving for $$\alpha$$ which is simple since it is a scalar, but then we would have to use matrix calculus to find $$\hat{\bf{v}}$$ such that the objective is minimized. There is another way. First off, we will use the result of so called <strong>*Best Approx. Theorem (1)*</strong> , which comes from known to everyone <strong>*Pythagorean Theorem*</strong>. This theorem tells us that:
{: style="text-align: justify"}

$$
\begin{align}
 \Vert\bf{x} - \hat{\bf{x}}\Vert_{2}^{2} \le \Vert\bf{x} - \bf{y}\Vert_{2}^{2},\ \forall{\bf{y}} \in Span\{\bf{v}\}
\end{align}
$$ 

where $$\hat{\bf{x}}$$ is an orthogonal projection of $$\bf{x}$$ onto $$Span\{\bf{v}\}$$ defined by $$\hat{\bf{x}} = \frac{\langle \bf{x}, \bf{v} \rangle}{\langle \bf{v}, \bf{v} \rangle} \bf{v}$$, but we have put a constraint that $$\bf{v}$$ is a unit vector, thus $$\hat{\bf{x}} = \langle  \bf{x}, \bf{v} \rangle \bf{v}$$.
{: style="text-align: justify"}

If you look closely you will see that we have solved half of the PCA objective. Since $$\Vert\bf{x} - \hat{\bf{x}}\Vert_{2}^{2} \le \Vert\bf{x} - \bf{y}\Vert_{2}^{2}$$ it must also be the case that $$\Vert\bf{x} - \hat{\bf{x}}\Vert_{2}^{2} \le \underset{\bf{y} \in Span\{\bf{v}\}}{\operatorname{min}}  \Vert\bf{x} - \bf{y}\Vert_{2}^{2}$$, and so $$\hat{\alpha} = \langle  \bf{x}, \bf{v} \rangle$$ is the solution.  Going back into our objective and substituting for $$\hat{\alpha}$$ we get:

$$
\begin{align}
 & \hat{\bf{v}}:= 
 \underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmin}}
	\Vert\bf{x}-\langle  \bf{x}, \bf{v} \rangle \bf{v}\Vert_{2}^2
	= \underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmin}}
	\Vert\bf{x}\Vert_{2}^2 + (\langle  \bf{x}, \bf{v} \rangle)^2\Vert\bf{v}\Vert_{2}^2 - 2(\langle  \bf{x}, \bf{v} \rangle)^2
 & = \underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmin}} - (\langle  \bf{x}, \bf{v} \rangle)^2
	=
	\underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmax}}(\langle  \bf{x}, \bf{v} \rangle)^2
	=\underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmax}}\bf{v}^{T} \bf{x} \bf{x}^{T} \bf{v}
\end{align}
$$ 

Where we omit $$\Vert\bf{x}\Vert_{2}^{2}$$ as it does not depend on $$\bf{v}$$, also since $$\bf{v}$$ is a unit vector $$\Vert\bf{v}\Vert_{2}^{2}$$=1, and we cancel out one of $$(\langle  \bf{x}, \bf{v} \rangle)^2$$. But the question "what is $$\bf{v}$$?" still remains.
#### 3.2. Expected value of random vector
So, intuitively $$\bf{v}$$ should be somehow related to our data point, but what should it be? Say, we got our data point $$\bf{x}$$ by the means of some measurement during an experiment, then $$\bf{x}$$ is a realization of some random vector $$\mathbb{X}$$. We are interested not in  minimizing the distance between one realization of $$\mathbb{X}$$ and $$\bf{v}$$ but in minimizing the expected value of this distance, thus the statistical PCA objective is:
$$(\hat{\bf{v}}, \hat{\alpha}):= 
\underset{
	\bf{v}, \ \alpha
	}
	{\operatorname{argmin}}
	\ \mathbb{E}_{\mathbb{X}} [\ \Vert\mathbb{X}-\alpha \bf{v}\Vert_{2}^2\ ] \\\ s.t. \ \Vert\bf{v}\Vert_{2}=1$$ We have already simplified this formula and so we can use it here, because the expected value is a linear transformation. Thus, we get:
$$\hat{\bf{v}}:=
\underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmax}} \ \mathbb{E}_{\mathbb{X}} [\bf{v}^{\bf{T}} \mathbb{X} \mathbb{X}^{\bf{T}} \bf{v}]
	=
	\underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmax}} \ \bf{v}^{\bf{T}} \mathbb{E}_{\mathbb{X}} [\mathbb{X} \mathbb{X}^{\bf{T}}]\bf{v}
	=
	\underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmax}} \ \bf{v^{T} \Sigma v}$$ Where we take $$\bf{v}$$ out of the expectation as it is constant if the expectation is taken w.r.t. $$\mathbb{X}$$. The last step comes from the definition of covariance of random vector, but we have assumed here that $$\mathbb{E}_{\mathbb{X}}[\mathbb{X}] = \bf{0_{n}}$$ and so $$\mathbb{E}_{\mathbb{X}} [\mathbb{X} \mathbb{X}^{T}] =Cov_{\mathbb{X}}(\mathbb{X}) = \bf{\Sigma}$$. We know that the covariance matrix is symmetric, and because it is obtained by multiplication of two random vectors it is also positive semi-definite, which means that $$\forall{\bf{x}} \in \mathbb{R}^n,\ \bf{x^{T} \Sigma x} \ge 0$$. Those two properties are the key for solving PCA objective without any calculus.

#### 3.3. Orthogonal eigendecomposition of covariance matrix
Every symmetric matrix attains an orthogonal eigendecomposition, if this matrix is also positive semi-definite, then all the eigenvalues have a property that $$\lambda_{i} \ge 0, \ i \in [1, n]$$. We write this decomposition as $$\bf{\Sigma} = \bf{PDP^{T}}$$, where $$\bf{D}$$ is a diagonal matrix with eigenvalues as entries and $$\bf{P}$$ is an orthogonal matrix such that $$\bf{P^{T}P} = \bf{I}_{n \times n}$$. The main convention is that eigenvectors of $$\bf{P}$$ are sorted so that their corresponding eigenvalues are placed in the decreasing order on the diagonal of $$\bf{D}$$. Very important here is to mention that those eigenvectors creates eigenbasis for $$\mathbb{R}^{n}$$, as this will allow us to solve for $$\bf{v}$$. With all that in mind we can write:
$$\underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmax}} \ \bf{v^{T} \Sigma v}
	=
\underset{
	\bf{v}, \ s.t. \ \Vert\bf{v}\Vert_{2}=1
	}
	{\operatorname{argmax}} \ \bf{v^{T} PDP^{T} v}$$
