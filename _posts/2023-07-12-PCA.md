---
layout: post
title: Solving PCA without matrix calculus and Lagrange multipliers
subtitle: We show that tou don't need calculus
tags: [test, PCA, linear algebra]
---

### 1. Introduction
Do you know what PCA is and does but you are unable to derive the results by yourself? Maybe you are stuck because of matrix calculus or maybe you don't know how to use Lagranglee multipliers, but you are just like me and the best way of understanding things  are by deriving them yourself. If you understand linear algebra the PCA can be solved without calculus. I will be using different theorems, that will be proven at the end of this blog, my knowledge on how to solve this problem comes from the great "Linear Algebra and its applications" by David C. Lay[1], which i highly recommend. I was trying to find similar articles or procedures, but with no success. If you know some that has taken the same route as in this blog, please link it in the comments and i will put it in the references.

#### 1.1. Notation
Vectors are denoted by bfface lowercase letters like $$\bf{x}$$, where we're using the convention that vectors are column vectors. We assume that any arbitrary vector comes from $$\mathbb{R}^{n}$$. Matrices are denoted by bfface uppercase letters like $$\bf{X}$$. Scalar values are denoted as usual. Random variables are denoted by uppercase letters like $$X$$ and random vectors are denoted as $$\mathbb{X}$$. The expected value w.r.t. random variable $$X$$ is denoted as $$\mathbb{E}_{X}[ \cdot ]$$. The $$\mathbb{l}2$$-norm is denoted as $$|| \bf{x} ||_{2}$$ and is defined as an inner product $$\langle\bf{x}, \bf{x}\rangle = \bf{x}^{T}\bf{x}$$. 

### 2. Outline of the objective problem
The basic premise behind PCA is to project some high-dimensional data onto a subspace generated by a set of vectors. We can define the objective function of PCA in two ways, where the second (*maximization*) comes naturally from the first (*minimization*), and so we will start by outlining the most basic definition. At the beginning we only consider one data point (a single vector) $$\bf{x} \in \mathbb{R}^{n}$$ to build some intuition and to simplify the calculations. Then, we can take any arbitrarily big dataset $$\bf{X} \in \mathbb{R}^{m \times n}$$ and proceed in the similar manner. We will also only take a look at projecting our datapoint onto a 1-dimentional subspace. Projecting onto a higher dimensional subspaces will be described later.

The PCA objective function tries to minimize the distance between our data vector $$\bf{x}$$ and the subspace spanned by some vector $$\bf{v} \in \mathbb{R}^{n}$$, which we denote as $$Span\{\bf{v}\}$$. We put a constraint, that $$\bf{v}$$ must be a unit vector, as this will only assure the unique solution. Since, minimizing the squared distance will give us the same result, we will be using it to write our objective, which turns out to be:

$$(\hat{\bf{v}}, \hat{\alpha}):= 
\underset{
	\bf{v}, \ \alpha
	}
	{\operatorname{argmin}}
	||\bf{x}-\alpha \bf{v}||_{2}^2 \\\ s.t. \ ||\bf{v}||_{2}=1$$ 

### 3. Solving 
#### 3.1. Solving for $$\alpha$$ 
Normally we would start with solving for $$\alpha$$ which is simple since it is a scalar, but then we would have to use matrix calculus to find $$\hat{\bf{v}}$$ such that the objective is minimized. There is another way. First off, we will use the result of so called <strong>*Best Approx. Theorem (1)*</strong> , which comes from known to everyone <strong>*Pythagorean Theorem*</strong>. This theorem tells us that:

$$||\bf{x} - \hat{\bf{x}}||_{2}^{2} \le ||\bf{x} - \bf{y}||_{2}^{2},\ \forall{\bf{y}} \in Span\{\bf{v}\}$$ 

where $$\hat{\bf{x}}$$ is an orthogonal projection of $$\bf{x}$$ onto $$Span\{\bf{v}\}$$ defined by $$\hat{\bf{x}} = \frac{\langle \bf{x}, \bf{v} \rangle}{\langle \bf{v}, \bf{v} \rangle} \bf{v}$$, but we have put a constraint that $$\bf{v}$$ is a unit vector, thus $$\hat{\bf{x}} = \langle  \bf{x}, \bf{v} \rangle \bf{v}$$.


If you look closely you will see that we have solved half of the PCA objective. Since $$||\bf{x} - \hat{\bf{x}}||_{2}^{2} \le ||\bf{x} - \bf{y}||_{2}^{2}$$ it must also be the case that $$||\bf{x} - \hat{\bf{x}}||_{2}^{2} \le \underset{\bf{y} \in Span\{\bf{v}\}}{\operatorname{min}} ||\bf{x} - \bf{y}||_{2}^{2}$$, and so $$\hat{\alpha} = \langle  \bf{x}, \bf{v} \rangle$$ is the solution.  Going back into our objective and substituting for $$\hat{\alpha}$$ we get:

$$\hat{\bf{v}}:= 
\underset{
	\bf{v}, \ s.t. \ ||\bf{v}||_{2}=1
	}
	{\operatorname{argmin}}
	||\bf{x}-\langle  \bf{x}, \bf{v} \rangle \bf{v}||_{2}^2
	=
\underset{
	\bf{v}, \ s.t. \ ||\bf{v}||_{2}=1
	}
	{\operatorname{argmin}}
	||\bf{x}||_{2}^2 + (\langle  \bf{x}, \bf{v} \rangle)^2||\bf{v}||_{2}^2 - 2(\langle  \bf{x}, \bf{v} \rangle)^2$$
	$$=
	\underset{
	\bf{v}, \ s.t. \ ||\bf{v}||_{2}=1
	}
	{\operatorname{argmin}} - (\langle  \bf{x}, \bf{v} \rangle)^2
	=
	\underset{
	\bf{v}, \ s.t. \ ||\bf{v}||_{2}=1
	}
	{\operatorname{argmax}}(\langle  \bf{x}, \bf{v} \rangle)^2
	=
\underset{
	\bf{v}, \ s.t. \ ||\bf{v}||_{2}=1
	}
	{\operatorname{argmax}}\bf{v}^{T} \bf{x} \bf{x}^{T} \bf{v}$$ 
 
Where we omit $$||\bf{x}||_{2}^{2}$$ as it does not depend on $$\bf{v}$$, also since $$\bf{v}$$ is a unit vector $$||\bf{v}||_{2}^{2}$$=1, and we cancel out one of $$(\langle  \bf{x}, \bf{v} \rangle)^2$$.

But the question "what is $$\bf{v}$$?" still remains.
