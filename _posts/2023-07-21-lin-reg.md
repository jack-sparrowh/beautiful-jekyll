---
layout: post
title: IN PROGRESS Prediction Error of Statistical Linear Regression
subtitle: It's more than just a line
tags: [statistics, probability, random vectors, linear regression, expected value, prediction error]
---
## 1. Few words about myself
I come from a background of Chemical Engineering, so for the good chunk of my life the linear regression was the REGLINP function in Excel, easy set and done. This is what i used during my university years. I always looked down on the linear regression, i thought it was so simple and not worthy of anyones time. I never knew how wrong I was. Over the past two years of self-study I grew to appreciate how beautiful the linear regression is, and most importantly, how complex it is. It was almost a spiritual experince (that humbled me to the core) to uncover completely new depth of this “simple” model, and it is all thanks to statistics. For this reason i recognise two types of linear regression, the one i used and neglected, and the statistical one. Thus, the title of this blog.  

## 2. Notation
In a typical fashion we let scalar values be denoted as lowecase letters $x$. Any arbitrary vetor is denoted by bold lowecase letters $\mathbf{x}$ and comes form $\mathbb{R}^{n}$. Any arbitrary matrix is denoted by bold uppercase letters $\mathbf{X}$, and matrices come from $\mathbb{R}^{m \times n}$ unless stated otherwise. Random variables are denoted as uprecase letters $X$, random vectors are denoted as $\mathbb{X}$, and random matrices are denoted as $\mathcal{X}$. Symbols with hat ( $\hat{.}$ \) are estimators or estimates. There are many ways to denote PDFs, here we will use the notation $f_{X}(x)$. What about PMFs? We change them to PDFs using "delta-train" as $f_{X}(x) = \sum_{k \in X(\Omega)} \mathcal{P} (X = k) \delta(x - k)$ and we don't think about them any more. The $l2$-norm for arbitrary vector space $V$ is denoted as $|| \cdot ||^{2}$. Note here, that $\mathcal{P}$ is not a power set it is a probability. I'm usually using $\mathbb{P}$, but i decided not to for readability puproses. I think that it is all we need. When we will use something not stated here, it will be clearly explained. 

### 2.1. Shape of $\textbf{X}$
So, in **2.** we have defined $$\mathbf{X}$$ to be a $$m \times n$$ matrix of data points. I'm always treating rows to be observations and columns to be features. For example, if $$\mathbf{X}$$ describes a medical data, rows would be patients and columns would be different parameters, like height, age, blood pressure etc. I know that some books and courses take different approach and consider the dataset to be $$\mathbf{X}^{T}$$. So, if $$\mathbf{x}_{i}$$ is an $$i$$th observation in the dataset, we define the dataset as:
{: style="text-align: justify"}

$$
\begin{align}
 \mathbf{X} = 
 \begin{pmatrix} 
 \mathbf{x}_{1}^{T} \\
 \vdots \\
 \mathbf{x}_{n}^{T}  
 \end{pmatrix}
 \in \mathbb{R}^{m \times n}
\end{align}
$$

## 3. Complaining about the expected value
There are handful of examples where authors use the notation for expected value i really love [… references…], which is, to write in subscript with respect to what random variable we are taking the expectation. So, instead of $\mathcal{E}[g(X, Y)]$ we would write $\mathcal{E}_{X, Y}[g(X, Y)]$. I cannot understand why isn't it much more popular in the field, especially in introductory books or when we are dealing with more than just one random variable under the expectation sign. When using this notation, one cannot just skip steps, or omit the assumtions and proceed further, because it is immediately obvious. The expected value is a function, just like any other, the only difference (compared to standard calculus classes) is that it utilizes the Lebesgue integral, since for any probability space {$\Omega$, $\mathcal{F}$, $\mathcal{P}$} and some arbitrary random variable $X: \Omega \to \mathbb{R}$, we define the expected value as:

$$
\begin{align}
 \mathcal{E}_{X} \left[ X \right] = \int_{\Omega} X d \mathcal{P} \Rightarrow 
 \int_{\mathbb{R}} x f_{X}(x) dx
\end{align}
$$

The defining equation is more or less abstract, but it implies the form we have all seen before. The most important thing is that $X$ in the subscript of $\mathcal{E}$ tells us that the integration is carried out using $f_{X}(x)dx$. So, here is a little task for you. If $\mathbb{Y}$ is some arbitrary random vector, $\mathcal{X}$ is some arbitrary random matrix, with joint probability $\mathcal{P}(\mathbb{Y}, \mathcal{X})$, $\mathcal{D}$ is the random dataset sample and  $\hat{\beta}(\mathcal{D})$ is an estimator for model parameter. Try to write  expected value below in the integral form.

$$
\begin{align}
 \mathcal{E} \left[|| \mathbb{Y} - f_{\hat{\beta}(\mathcal{D})} (\mathcal{X}) || ^ {2} \right]
\end{align}
$$

One might say that the integral form depends on the context of the discussion, of course, but as you will see it is very easy to solve it with whatever random variables you want, for example by ommiting the fact that $\hat{\beta}(\mathcal{D})$ depends on elements of $\mathcal{D}$, and consider just some of those elements. If you are deep into statistics you can understand what's going on, but for people starting in statistics or trying to learn new things it can take a lot of time to decipher what is happening from one step to another. This is especially frustrating as the notation from one book to another, from one article to another is not consistent. Thus, I want to be as clear as possible in this blog, and i'm going to be straight honest with you right now, 

<strong>i'm not 100% sure about some of the results, as i have failed miserably to find any supporting texts on this topic with clear and consistent solution, please keep that in mind. My derivations will be clearly indicated. All the other results will be marked by an appropriate sorce</strong>. 

So, you might ask "What is the purpose of this blog then?", and to be honest i think that the questions asked here are very interesting when it comes down to solving (or trying to solve) the statistical linear regression.

## 4. Why even bother?
Minimization of some empirical risk function, by finding the right hyposhesis (which is a fancy word for model) is the main goal of machine learning. If we let average value of $l2$-norm to be our risk function and linear regression to be the hypothesis, then by assuming some characteristics of added error it is fairly easy to find not only the vector parameter for linear model, but other interesting relations. The OLS solution should be known to everyone interested in statistics. But, just to remind you, if $\mathbf{y} \in \mathbb{R}^{m}$ and $\mathbf{X'} = \left[ \mathbf{1}_{m} : \mathbf{X} \right] \in \mathbb{R}^{m \times n + 1}$, the OLS estimator is:

$$
\begin{align}
 \hat{\beta}_{OLS} = \left( \mathbf{X'}^{T} \mathbf{X'} \right)^{-1} \mathbf{X'}^{T} \mathbf{y} \in \mathbb{R}^{n+1}
\end{align}
$$

If you have no idea where did it come from, at the end I'm providing two ways of deriving this result, one using matrix calculus and the other one using linear algebra (no calculus needed!).

### 4.1. Assumtion about the true relation (should it be under chapter 4?)
Our main assumption is that there exist a true relation between the target and vector of observations with added normally distributed error (it doesn't have to be normal, but for simplicity we let it be that way):

$$
\begin{align}
 y = f(\mathbf{x}) + e, \ e \sim N(0, \sigma^{2})
\end{align}
$$

This is the simplest, and I would say, the most fundamental assumtion in machine learning. We state that there exists a relation that connects the target variable with measured variables, but we know that we cannot measure ALL possible variables, and that the measurement precission is finite. Thus, we accept the fact that a random error is going to be added to the true value, which give a rise of the joint probability distribution over the possible outcomes space $\Omega_{Y} \times \Omega_{\mathbb{X}}$, namely $\mathcal{P} (Y, \mathbb{X})$ (which will be later denoted as $f_{Y, \mathbb{X}}(y, \mathbf{x}))$.

### 4.2. Think about
Everyday I'm carrying out the same reactions, for each reaction $i$ collecting yield $y_{i}$ for main product (targets) and writing down values of the parameters, like time at which the sample was sampled(?), temperature, the masses of substrates put into the reactor, ect., which are my observations $\mathbf{x}_{i}$. But every time I'm running the reaction I preciselly know how $\mathbf{x}_{i}$ is going to look like, since I'm the one choosing parameters, so there is nothing random about any of $\mathbf{x}_{i}$! This means that the distribution of any chosen set of parameters is collapsing to the 

$$
\begin{align}
\end{align}
$$
