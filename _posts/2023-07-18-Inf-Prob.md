---
layout: post
title: Finding probabilities on infinitesimal regions of arbitrary random vectors
subtitle: Probably it doesn't matter
thumbnail-img: /assets/img/inft-prob.png
tags: [statistics, probability, random vectors]
---

### 1. Introduction
I would like to start off by stating that I'm no expert when it comes down to Measure Theory, to say the least. I'm trying to make my way through Tom Lindstr√∏m's "Introduction to Real Analysis", but it takes time to wrap one's head around real analysis.
{: style="text-align: justify"}

So, what am I trying to do here? If you are familiar with probability you should remember the relation $$\mathbb{P}[x \le X \le x + \Delta{x}] \approx f_{X}(x) \Delta{x}$$, where of course, $$X$$ is a random variable and $$f_{X}(x)$$ is a PDF. This is proven in some books, skimmed by in others. Eventhough you can find the proof yourself i couldn't find one for joint probability of two random variables, for three r.v., and not even mentioning a random vector of an arbitrary size. Thus, i would like to derive those relations. 
{: style="text-align: justify"}

### 1.1. Assumption

To make matters clear, we are here using only differentiable and integrable functions, we don't think if the function is Riemann integrable or Lebesgue integrable, it just is. We are dealing only with PDFs which satisfies those assumptions. 
{: style="text-align: justify"}

### 2. Univariate case

This is basic, and can be easily found online or in any decent (or even not so much) book on probability theory, but i will derive it for completeness. We let our random variable be defined on some arbitrary region in $$\mathbb{R}$$, by stating $$X(\Omega) = [a, b]$$, with $$a < b$$. If we let $$\Delta{x} > 0$$ we define the probability of $$X$$ falling in a region $$[x, x + \Delta{x}] \subseteq [a, b]$$ as:
{: style="text-align: justify"}

$$
\begin{align}
 \mathbb{P}[x \le X \le x + \Delta{x}] = \int_{x}^{x + \Delta{x}}f_{X}(t)dt
\end{align}
$$ 

This is true for both continuous distributions as well as descrete ones if we represent them using so-called "delta train", $$f_{X}(x) = \sum_{k \in X(\Omega)} \mathbb{P}[X=k] \delta(x-k)$$. Thus, given that $$f_{X}(x)$$ is a PDF there must exist a corresponding CDF, where  both are related by:
{: style="text-align: justify"}

$$
\begin{align}
 f_{X}(x) = \frac{d F_{X}(x)}{dx}
\end{align}
$$ 

This shows that CDF is an antiderivative of PDF, and using <strong>*The Fundamental Theorem of Calculus*</strong> we show that,
{: style="text-align: justify"}

$$
\begin{align}
 \int_{x}^{x + \Delta{x}}f_{X}(t)dt = F_{X}(x + \Delta{x}) - F_{X}(x)
\end{align}
$$ 

Now, by <strong>The Mean Value Theorem</strong> there exist some value $$x^{*} \in [x, x + \Delta{x}]$$ such that:

$$
\begin{align}
 F_{X}(x + \Delta{x}) - F_{X}(x) = \frac{d F_{X}(x^{*})}{dx} \Delta{x} = f_{X}(x^{*}) \Delta{x}
\end{align}
$$ 

Putting this together we see that,

$$
\begin{align}
 \mathbb{P}[x \le X \le x + \Delta{x}] = f_{X}(x^{*}) \Delta{x}
\end{align}
$$ 

Because, we don't know what $$x^{*}$$ is, we approximiate this result by $$x$$, obtaining the final relation,

$$
\begin{align}
 \mathbb{P}[x \le X \le x + \Delta{x}] \approx f_{X}(x) \Delta{x}
\end{align}
$$

### 3. Bivariate case

Here we are dealing with two random variables $$X$$ and $$Y$$, defined on some arbitraty region, which we denote as $$X(\Omega_{X}) \times Y(\Omega_{Y}) = [a_{1}, b_{1}] \times [a_{2}, b_{2}]$$, with $$(a_{1}, a_{2}) \prec (b_{1}, b_{2})$$. Now, as in the univariate case, let $$\Delta{x} > 0$$ and $$\Delta{y} > 0$$, then we define the probability of $$(X, Y)$$ falling in a rectangle $$[x, x + \Delta{x}] \times [y, y + \Delta{y}] \subseteq [a_{1}, b_{1}] \times [a_{2}, b_{2}]$$ as,
{: style="text-align: justify"}

$$
\begin{align}
 \mathbb{P}[x \le X \le x + \Delta{x}, y \le Y \le y + \Delta{y}] = 
 \int_{x}^{x + \Delta{x}} \int_{y}^{y + \Delta{y}} f_{X, Y}(t_{1}, t_{2}) dt_{2} dt_{1}
\end{align}
$$

We want to show that this approximiates $$f_{X,Y}(x, y) \Delta{x} \Delta{y}$$. Let's consider the simplest case, that $$X$$ and $$Y$$ are independent (they don't have to be $$i.i.d.$$), then
{: style="text-align: justify"}

$$
\begin{align}
 \int_{x}^{x + \Delta{x}} \int_{y}^{y + \Delta{y}} f_{X, Y}(t_{1}, t_{2}) dt_{2} dt_{1} = 
 \int_{x}^{x + \Delta{x}} f_{X}(t_{1}) dt_{1} \int_{y}^{y + \Delta{y}} f_{Y}(t_{2}) dt_{2}
\end{align}
$$

$$
\begin{align}
 \approx f_{X}(x) f_{Y}(y) \Delta{x} \Delta{y} = f_{X,Y}(x, y) \Delta{x} \Delta{y}
\end{align}
$$

We see that if random variables are indeed independent, then this approximation is true. The univariate case is proven using the relation between PDF and CDF, and so it might be a good idea to try it out here. We could use <strong>The Fundamental Theorem of Calculus</strong> two times in a row, but using the simple relation between PDF and CDF can save us a lot of lines of equations. The relation in question is:
{: style="text-align: justify"}

$$
\begin{align}
 f_{X, Y}(x, y) = \frac{\partial^{2} F_{X, Y} (x, y)}{\partial{x} \partial{y}}
\end{align}
$$

Let us substitute this relation into our probability definition:

$$
\begin{align}
 \int_{x}^{x + \Delta{x}} \int_{y}^{y + \Delta{y}} f_{X, Y}(t_{1}, t_{2}) dt_{2} dt_{1} =
 \int_{x}^{x + \Delta{x}} \int_{y}^{y + \Delta{y}} 
 \left. \frac{\partial^{2} F_{X, Y} (x, y)}{\partial{x} \partial{y}} \right|_{(x, y) = (t_{1}, t_{2})} 
 dt_{2} dt_{1} 
\end{align}
$$

For more compact notation we rewrite the last step substituting $$t_{1}$$ and $$t_{2}$$ for $$x, y$$ respecitvely.

$$
\begin{align}
 \int_{x}^{x + \Delta{x}} \int_{y}^{y + \Delta{y}} 
 \left. \frac{\partial^{2} F_{X, Y} (x, y)}{\partial{x} \partial{y}} \right|_{(x, y) = (t_{1}, t_{2})} 
 dt_{2} dt_{1} =
 \int_{x}^{x + \Delta{x}} \int_{y}^{y + \Delta{y}} 
 \frac{ \partial^{2} F_{X, Y} (t_{1}, t_{2}) }{ \partial{t_{1} } \partial{ t_{2} }} dt_{2} dt_{1}
\end{align}
$$

We can now split the second derivative and proceede with the calculations as it was a CDF of a single random variable:

$$
\begin{align}
 \int_{x}^{x + \Delta{x}} \int_{y}^{y + \Delta{y}} 
 \frac{ \partial^{2} F_{X, Y} (t_{1}, t_{2}) }{ \partial{t_{1} } \partial{ t_{2} }} dt_{2} dt_{1} =
 \int_{x}^{x + \Delta{x}} \frac{\partial}{\partial{t_{1}}} 
 \left[ \int_{y}^{y + \Delta{y}} \frac{\partial F_{X, Y} (t_{1}, t_{2}) }{\partial{t_{2}}} dt_{2} \right] dt_{1} 
\end{align}
$$

We can take what is inside the parentheses and focus only on that particular integral:

$$
\begin{align}
 \int_{y}^{y + \Delta{y}} \frac{\partial F_{X, Y} (t_{1}, t_{2}) }{\partial{t_{2}}} dt_{2}=
 F_{X, Y} (t_1, y + \Delta{y}) - F_{X, Y} (t_1, y)
\end{align}
$$

This is a simple relation, especialy if you think of it as a function of one variable. By <strong>The Mean Value Theorem</strong> there exist $$y^{*} \in [y, y + \Delta{y}]$$ such that,
{: style="text-align: justify"}

$$
\begin{align}
 F_{X, Y} (t_1, y + \Delta{y}) - F_{X, Y} (t_1, y) = \frac{\partial F_{X, Y} (t_{1}, y^{*})}{\partial y} \Delta{y}
\end{align}
$$

and so we can substitute this result into a parentheses in the probability integral, and we get

$$
\begin{align}
 \int_{x}^{x + \Delta{x}} \frac{\partial}{\partial{t_{1}}} 
 \left[\frac{\partial F_{X, Y} (t_{1}, y^{*})}{\partial y} \Delta{y} \right] dt_{1} =
 \Delta{y} \frac{\partial}{\partial y} 
 \int_{x}^{x + \Delta{x}} \frac{\partial F_{X, Y} (t_{1}, y^{*})}{\partial{t_{1}}} dt_{1}
\end{align}
$$

The last step is due to the symmetry property of second derivatives, which comes from <strong>The Clairaut's Theorem</strong>.
