---
layout: post
title: Interesting solution for Multistage Compression with Intercooling
subtitle: ???
thumbnail-img: /assets/img/PCA_equation.png
tags: [PCA, linear algebra]
---

### 1. Introduction
Do you know what PCA is and does but you are unable to derive the results by yourself? Maybe you are stuck because of matrix calculus or maybe you don't know how to use Lagrange multipliers, but you are just like me and the best way of understanding things is by deriving them yourself. If you understand linear algebra the PCA can be solved without calculus. I will be using different theorems, that will be proven at the end of this blog. My knowledge on how to solve this problem comes from the great "Linear Algebra and its applications" by David C. Lay[1], which i highly recommend. I was trying to find similar articles or procedures, but with no success. If you know some that has taken the same route as in this blog, please link it in the comments (which are disabled since i haven't figured it out yet) and i will put it in the references.
{: style="text-align: justify"}

#### 1.1. Notation
Vectors are denoted by boldface lowercase letters like $$\bf{x}$$, where we're using the convention that vectors are column vectors. We assume that any arbitrary vector comes from $$\mathbb{R}^{n}$$. Matrices are denoted by boldface uppercase letters like $$\bf{X}$$. Scalar values are denoted as x. Random variables are denoted by uppercase letters like $$X$$ and random vectors are denoted as $$\mathbb{X}$$. The expected value w.r.t. random variable $$X$$ is denoted as $$\mathbb{E}_{X}[ \cdot ]$$. The $$\mathbb{l}2$$-norm is denoted as $$\Vert \bf{x} \Vert_{2}$$ and is defined as an inner product $$\langle\bf{x}, \bf{x}\rangle = \bf{x}^{T}\bf{x}$$.
{: style="text-align: justify"}

### 2. Outline of the objective problem
The basic premise behind PCA is to project some high-dimensional data onto a subspace generated by a set of vectors. We can define the objective function of PCA in two ways, where the second (*maximization*) comes naturally from the first (*minimization*), and so we will start by outlining the most basic definition. At the beginning we only consider one data point (a single vector) $$\bf{x} \in \mathbb{R}^{n}$$ to build some intuition and to simplify the calculations. Then, we can take any arbitrarily big dataset $$\bf{X} \in \mathbb{R}^{m \times n}$$ and proceed in the similar manner. We will also only take a look at projecting our datapoint onto a 1-dimentional subspace. Projecting onto a higher dimensional subspaces will be described later.
{: style="text-align: justify"}

The PCA objective function tries to minimize the distance between our data vector $$\bf{x}$$ and the subspace spanned by some vector $$\bf{v} \in \mathbb{R}^{n}$$, which we denote as $$Span\{\bf{v}\}$$. We put a constraint, that $$\bf{v}$$ must be a unit vector, as this will only assure the unique solution. Since, minimizing the squared distance will give us the same result, we will be using it to write our objective, which turns out to be:
{: style="text-align: justify"}

$$
\begin{aligned}
 (\hat{\bf{v}}, \hat{\alpha}):= 
 \underset{\bf{v}, \ \alpha, \ s.t. \ \Vert\bf{v}\Vert_{2}=1}{\operatorname{argmin}}\Vert\bf{x}-\alpha \bf{v}\Vert_{2}^2 \\
\end{aligned}
$$ 



### 7. References
[1]	D. C. Lay, Algebra and its applications 5 ED, vol. 91, no. 1. 2011. \\
[2]	“MNIST database.” [link](https://en.wikipedia.org/wiki/MNIST_database "MNIST")  \\
[3]	“sklearn.decomposition.PCA.” [link](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html "Sklearn PCA")
